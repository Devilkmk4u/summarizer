{"name":"Summarizer","tagline":"Text Summarization using LSA and TextRank in Apache Spark","body":"# Summarizer - Summarize Product Reviews\r\n\r\n## Table of Contents\r\n - [Introducntion](#introduction)\r\n - [Dataset](#dataset)\r\n    - [Data Collection](#data-collection)\r\n    - [Data Preparation](#data-preparation)\r\n - [Methods](#methods)\r\n    - [Latent Semantic Analysis](#latent-semantic-analysis)\r\n    - [Text Rank](#textrank)\r\n - [Demos](#online-demos)\r\n - [Installation](#installation)\r\n - [Execution Instruction](#execution-instruction)\r\n - [Source Code](#source-code)\r\n - [Reference](#reference)\r\n\r\n## Introduction\r\nThe idea of this project is to build a model for e-commerce data that summarize large amount of customer reviews of a \r\nproduct to give an overview about the product. The result of this model can be used to get an overview of what are the \r\nmost important reveiws that many customers complaining or praising about a particular product without reading all of \r\nthe reviews.\r\n\r\n## Dataset\r\n### Data Collection\r\nThe dataset used for this project is crawled from Amazon.com. The dataset contains products' reviews in separate files\r\nfor each product, each file contains maximum of 1000 reviews. Since the reviews are sorted by ranking, the first thousand\r\nreviews are more than sufficient for the summarization task. Each review in a file contains `review_id`, `ratings`, \r\n`review_title`, `helpful_votes`, `total_votes` and `full_review`. The reviews file for a product is named by its \r\n`product_id` and all the metadata about the products are stored in a file called `iteminfo.txt`.\r\n\r\n**Note:** Data is collected using the [customer-review-crawler](https://github.com/iamprem/customer-review-crawler) which is\r\n**forked from [maifeng's crawler](https://github.com/maifeng/customer-review-crawler)** written in java. The original\r\nversion was outdated, so i had to rewrite the whole code that are used for data collection for this project. Please see\r\nthe commit history for the changes that I made to the forked version.\r\n\r\n![Commit log comparison between maifen and myself(iamprem)](https://raw.githubusercontent.com/iamprem/temp/master/assets/commit_tree.png)\r\n#### Description of dataset\r\n    \r\n**product_id.txt**  -- a file that contains reviews about a product.\r\n\r\n    review_id       -   Unique id given to a review\r\n    ratings         -   Integer value ranges from 1 to 5, describes rating of the product\r\n    review_title    -   Punch line given by the reviewer for their review\r\n    helpful_votes   -   Number of people found the review was helpful\r\n    total_votes     -   Number of people upvoted or downvoted the review\r\n    full_reveiw     -   Full review given by the reviewer\r\n\r\n**itemsinfo.txt**   -- a file that contains all the product metadata\r\n\r\n    product_id      -   Unique id for a product\r\n    product_name    -   Listed name for the product in Amazon.com\r\n    price           -   Price in US Dollars\r\n\r\n### Data Preparation\r\n#### Key Decisions in Data Preperation\r\n* Since the summarization task is extractive(not abstractive) from the original review file, only sentences with number \r\nof words between 10 and 30 are considered to avoid long story lines written by users in the final summary.\r\n* Only english alphabets are considered in the summarization process. All special characters and numbers are ignored in \r\nboth methods implemented in this project.\r\n* Stopwords in english are ignored and all other words are lemmatized.\r\n\r\n## Summarization Methods\r\n### Latent Semantic Analysis\r\nLSA Disription goes here  \r\nTake the positive and negative reviews based on the ratings and separate them as two sets of data for\r\neach product. Pre-process the data by tokenizing and removing the stop words, then do the Latent\r\nSemantic Analysis by doing the following. Compute TF-IDF matrix with words as rows and sentences as\r\ncolumns using Spark. After computing the TF-IDF matrix, factorize the matrix by Singular Value\r\nDecomposition (using numpy, since no python wrapper for MlLib’s implementation) and collect the key\r\nsentences from the right singular matrix. Collect the top ‘k’ key sentences and add it to the final\r\nsummary of the product.\r\n### TextRank\r\nTextrank Disription goes here  \r\nConstruct a Graph with sentences from reviews as vertices and the similarity between the sentences as\r\nthe weight of the edges. Non-overlapping sentences have zero weight on the edge between them and\r\nhighly overlapping sentences have high weights. This resulting graph would be a connected graph.\r\nImplement the graph based ranking algorithm called TextRank (similar to PageRank and HITS) in Spark\r\nand compute the final ranks of each sentence. Collect top ‘k’ ranked sentences and add it to the\r\nsummary.\r\n##Demos\r\n\r\n#### LSA in Action\r\nSummarization using Latent Semantic Analysis is shown below. Here for simplicity only two concepts are\r\nselected and in each concept five sentences are extracted. **Note the similarity between sentences in each\r\nconcept.**\r\n![Summarization using LSA](https://raw.githubusercontent.com/iamprem/temp/master/assets/lsa_exe.gif)\r\n\r\n#### TextRank in Action\r\n![Summary sentences using TextRank](https://raw.githubusercontent.com/iamprem/temp/master/assets/tr_exe.gif)\r\n## Installation\r\n\r\n### Dependencies\r\n\r\n* Python 2.6 or 2.7(not tested in 3.x)\r\n* [Install pip](http://pip.readthedocs.org/en/stable/installing/)\r\n* Numpy(version >1.4)\r\n* NLTK Library\r\n* Apache Spark\r\n\r\n#### Install Numpy and NLTK\r\n    sudo pip install -U numpy\r\n    sudo pip install -U nltk\r\n\r\n#### Download Stopwords from nltk data source\r\n     //Pythonic way\r\n     import nltk\r\n     nltk.download('all-corpora')\r\n            (or)\r\n     //Command line way       \r\n     python -m nltk.downloader all-corpora\r\n    \r\n    \r\n### Execution Instruction\r\n\r\n#### Summarization using LSA\r\n    spark-submit lsa.py -s <inputfile>\r\n\r\n#### Summarization using TextRank\r\n    spark-submit textrank.py <iter-count> <summary-sent-count> <inputfile>\r\n    \r\n## References\r\n1. Y. Gong and X. Liu. 2001. Generic text summarization using relevance measure and latent\r\nsemantic analysis. In Proceedings of SIGIR.\r\n2. R. Mihalcea and P. Tarau. TextRank - bringing order into texts. In Proceedings of the Conference\r\non Empirical Methods in Natural Language Processing (EMNLP 2004), Barcelona, Spain, 2004.\r\n3. G. Erkan and D. R. Radev (2004) \"LexRank: Graph-based Lexical Centrality as Salience in Text\r\nSummarization\", Volume 22, pages 457-479","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}